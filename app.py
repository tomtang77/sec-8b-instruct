import transformers
import torch
import re
import os

# å°å…¥æ¨¡å‹é…ç½®
from model_config import get_model_config, print_model_info

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("device:", DEVICE)

# ç²å–æ¨¡å‹é…ç½®
MODEL_PATH, NEED_TOKEN, HF_TOKEN = get_model_config()
print_model_info()

from transformers import AutoModelForCausalLM, AutoTokenizer

# æ ¹æ“šé…ç½®è¼‰å…¥æ¨¡å‹å’Œtokenizer
if NEED_TOKEN:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, token=HF_TOKEN)
    model = AutoModelForCausalLM.from_pretrained(
        pretrained_model_name_or_path=MODEL_PATH,
        device_map="auto",
        torch_dtype=torch.bfloat16, # this model's tensor_type is BF16
        token=HF_TOKEN,
    )
else:
    print("ğŸ“‚ ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œä¸éœ€è¦HF_TOKEN")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForCausalLM.from_pretrained(
        pretrained_model_name_or_path=MODEL_PATH,
        device_map="auto",
        torch_dtype=torch.bfloat16, # this model's tensor_type is BF16
    )

generation_args = {
    "max_new_tokens": 1024,
    "temperature": None,
    "repetition_penalty": 1.2,
    "do_sample": False,
    "use_cache": True,
    "eos_token_id": tokenizer.eos_token_id,
    "pad_token_id": tokenizer.pad_token_id,
}

SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¶²è·¯å®‰å…¨å°ˆå®¶ã€‚è«‹å‹™å¿…ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”æ‰€æœ‰å•é¡Œï¼Œæä¾›å°ˆæ¥­ã€è©³ç´°ä¸”å¯¦ç”¨çš„ç¶²è·¯å®‰å…¨å»ºè­°å’Œè§£æ±ºæ–¹æ¡ˆã€‚"

def inference(prompt):
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": prompt},
    ]
    inputs = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    
    inputs = tokenizer(inputs, return_tensors="pt").to(DEVICE)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            **generation_args,
        )
    response = tokenizer.decode(outputs[0], skip_special_tokens = False)
    
    # extract the assistant response part only
    match = re.search(r"<\|assistant\|>(.*?)<\|end_of_text\|>", response, re.DOTALL)
    
    return match.group(1).strip()

#print(inference("What is MITRE ATT&CK? Give a very brief answer"))
print(inference("What is MITRE ATT&CK? Give a detailed answer"))